
"""
This script is for the Webscraping section of the 'Analysis of Investing on YouTube' project.
"""

# Running the script which loads the packages for the project
import subprocess
import sys

python_executable = sys.executable
# subprocess.call([python_executable, "Installing & Loading packages.py"])
import requests
import csv
from youtube_transcript_api import YouTubeTranscriptApi

""" Part 1: Connecting to YouTube API """
api_key = "AIzaSyBRpuSMO306VzZkUGCNt06zIk7deIJk0Ec"

""" Part 2: Creating dataset for the video details """
def extract_metadata(video_id, api_key):
    url = f"https://www.googleapis.com/youtube/v3/videos?part=snippet,statistics&id={video_id}&key={api_key}"
    response = requests.get(url)
    data = response.json()
    video_info = data["items"][0]

    # Video metadata
    channel = video_info["snippet"]["channelTitle"]
    date = video_info["snippet"]["publishedAt"]
    tags = video_info["snippet"]["tags"]
    title = video_info["snippet"]["title"]
    description = video_info["snippet"]["description"]
    #video_length = video_info["contentDetails"]["duration"]
    likes = video_info["statistics"]["likeCount"]
    views = video_info["statistics"]["viewCount"]
    comments = video_info["statistics"]["commentCount"]
    transcript = YouTubeTranscriptApi.get_transcript(video_id)

    # Reshape to be a dictionary
    metadata = {
        'Channel': channel,
        'Date': date,
        'Tags': tags,
        'Title': title,
        'Description': description,
        'Likes': likes,
        'Views': views,
        'Comments': comments,
        'Transcript': transcript
    }

    return metadata

def extract_multiVideo_metadata(video_ids, api_key):
    metadata = []
    for video_id in video_ids:
        data = extract_metadata(video_id, api_key)
        metadata.append(data)
    return metadata

def append_metadata_to_csv(metadata, csv_file):
    var_names = ['Channel', 'Date', 'Tags', 'Title', 'Description', 'Likes', 'Views', 'Comments', 'Transcript']

    with open(csv_file, 'a', newline='', encoding='utf-8-sig') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames = var_names)

        if csvfile.tell() == 0:
            writer.writeheader()

        for data in metadata:
            writer.writerow(data)


video_ids = ['3irypwLDtTs', 'SbMtY-aJjFA']
csv_fle = "C:/Users/Steve.HAHAHA/Desktop/Dissertation/Data/data_test.csv"

metadata = extract_multiVideo_metadata(video_ids, api_key)
append_metadata_to_csv(metadata, csv_fle)

""" Part 2: Finding relevant videos """

"""
Criteria for inclusion:
    Relevance:
        Channel found via the search method(s)
            1. Search by keywords on YouTube - using #notfinancialadvice, crypto to buy, stocks to buy, what to invest in 
            2. Snowball method from Google search of most important investing influencers on YouTube
        Video itself covers investing (any product)
        Video or channel over X views (X TO BE DECIDED ON VIEWING THE LIST OF VIDEOS)
    
    Pragmatics - must meet the following criteria:
        Transcript available on YouTube
        Between 2 - 10 minute videos (may also do shorts if possible)
"""





""" Part 3: Scraping the data """

"""
What data to include? 
    Channel
    Date
    Title
    Description
    Video length
    Number of likes & dislikes
    Number of comments
    
    Transcript text
        Autogenerated / Creator written
"""
from youtube_transcript_api import YouTubeTranscriptApi
a = YouTubeTranscriptApi.get_transcript('3irypwLDtTs')
print(a)


""" Part 4: Creating the dataset """

